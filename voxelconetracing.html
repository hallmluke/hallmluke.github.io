<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Voxel Cone Tracing | Luke Hall</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<header id="header">
					<div class="inner">

						<!-- Home -->
						<a href="index.html" class="logo fas fa-home">
							<!-- <span class="symbol"><img src="images/logo.svg" alt="" /></span> -->
						</a>

						<!-- Nav -->
							<nav>
								<ul>
									<li><a href="#menu">Menu</a></li>
								</ul>
							</nav>

					</div>
				</header>

			<!-- Menu -->
				<nav id="menu">
					<h2>Menu</h2>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="about.html">About</a></li>
						<li><a href="portfolio.html">Portfolio</a></li>
					</ul>
				</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Voxel Cone Tracing</h1>
							<h4>Last Updated: 5/4/22</h4>
							<span class="image main"><img src="images/vctheader.png" alt="" /></span>
							<h2>What is Voxel Cone Tracing?</h2>
							<p>Voxel Cone Tracing (VCT) is a technique originally proposed by NVIDIA researchers in 2011 in <a href="https://research.nvidia.com/sites/default/files/pubs/2011-09_Interactive-Indirect-Illumination/GIVoxels-pg2011-authors.pdf">this paper.</a>
							This method of calculating indirect lighting has been mostly sidelined recently with the advent of hardware-accelerated raytracing (such as RTX) allowing more sophisticated methods of calculating GI--such as <a href="https://jcgt.org/published/0008/02/01/">ray-traced irradiance fields</a>, a topic I may wish to explore in the future.
							However, I had never implemented a GI system in a renderer before, and VCT interested me for a few reasons. First, I (sadly) do not have a GPU with hardware-accelerated raytracing at the moment. That eliminated many options for me. Similarly, I wanted a system that could run smoothly (~60 FPS) on older, mid-range hardware (such as my 1050Ti).
							Second, I found that while some implementation details are tricky, the concept of VCT is fairly intuitive. As such, I set out to implement a (simplified) version of the paper mentioned above.</p>

							<p>VCT can be broken down into two stages. The first is the voxelization of the scene and its illumination into memory. For those unfamiliar, a voxel is simply a value in a 3D grid. So to voxelize the scene is to take the scene and store all relevant information (illumination, normal) into this grid structure. This allows for 
								an efficient lookup for illumination at any given point. The second stage is to sample this data to calculate indirect illumination. For each pixel, in addition to the direct illumination calculations, we can perform additional calculations to sample nearby voxels for their illumination data, allowing us to light that pixel accordingly.
							</p>

							<h2>Voxelization</h2>
							<p>So how do we even begin take our scene and convert it into this grid structure? What part of the scene are we voxelizing? Moreover, what is this grid structure in memory? For the first question, we can actually make use of the fixed-function graphics pipeline. By that I mean, we can treat this almost like an additional render pass, but instead of
								our output being a framebuffer texture target, our fragments can instead store values in a buffer. Below is a small snippet of a GLSL fragment shader that demonstrates what I mean.
								<pre><code>
...							
struct VoxelType
{
    uint color;
    uint normal;
};

layout (std140, binding = 1) buffer VoxelBuffer {
    VoxelType voxels[];
};

...

void main() {

    ivec3 voxel = scaleAndBias(FragPos);

	... 
    uint id = flatten3D(uvec3(voxel), uvec3(GISubdiv));
    atomicMax(voxels[id].color, encodedColor);
    atomicMax(voxels[id].normal, encodedNormal);

}
								</code></pre>

							In the above code, for each fragment we're storing values using the atomicMax operation (more on this later) in this buffer that is representative of our voxel grid. Based on the position of our fragment, we can index into this buffer and store our information. In this example, we're storing an encoded value of our color and the normal of the surface.
							</p>

							<p>I mentioned that we could use the fragment's global position to index into this structure. How do we go from a global position of a fragment to an index in this buffer? Well, it depends on your desired implementation. There are two ways to think about where this grid lives in 3D space.
								The first is to have this grid be relative to the current view/camera. There are several possible advantages to this. We would not need to manually place bounds for our voxel. We would only be voxelizing data we need for the current frame, not values that may not get seen. However, this would require re-voxelization every frame, as any change in view would invalidate our previous voxel data.
								The second, and the method I chose that is also used by <a href="https://docs.godotengine.org/en/latest/classes/class_voxelgi.html">popular game engine Godot</a>, is to place "probes" (bounds for the grid) into the scene manually for regions we wish to voxelize. This also allows us to simply voxelize offline or at load-time, greatly reducing the computation required. This, of course, has the obvious drawback that GI is static in the scene. Depending on the capability of the underlying hardware the renderer could either
								use the "static" data or revoxelize every frame. My definition of a probe is fairly simple:
<pre><code>
class GIProbe {
	public:
		std::shared_ptr&#60;Texture3D&#62; VoxelTex;
		uint32_t Subdiv = 128;
		float Energy = 1.0f;

		GIProbe() = default;
};
</code></pre>
							The above is a component that can be attached to a scene object with its own position/scale. The position/scale represent where it is in the scene. It contains a pointer to a 3D texture that the previous buffer is converted into (more on this later). The "Subdiv" represents the level of detail
							of the probe. For example, a probe with 128 Subdiv would be a 128 x 128 x 128 voxel grid. The "Energy" is a modifier to increase the effect of the indirect lighting for stylized/physically implausible results.
							</p>

							<p>Now that we know the probe's position, scale and subdivisions, we can begin the voxelization process. We first construct an orthographic projection matrix that covers the bounds of the voxel probe and a view matrix looking toward the center of the probe in the -Z direction.
								This is fairly trivial to do using GLM's ortho and lookat functions:
<pre><code>
glm::mat4 voxelProjection = glm::ortho(-scales[i].x, scales[i].x, -scales[i].y, scales[i].y, -scales[i].z, scales[i].z);
glm::mat4 voxelView = glm::lookAt(glm::vec3(0.0f, 0.0f, scales[i].z / 2), positions[i], glm::vec3(0.0f, 1.0f, 0.0f));
</code></pre>

							However, we have a problem. Rendering the scene from only a single view will fail to produce fragments for some triangles that should be accounted for. For example, consider a triangle whose normal is straight up. The graphics pipeline will not produce fragments for this triangle,
							and as such, will not store a value in the buffer. A naive solution would be to render the scene from three directions for each axis. This would work but would be incredibly inefficient. A better solution is to project each triangle onto its dominant axis in a geometry shader
							such that each triangle produces the maximum number of fragments. We should ensure that the actual position of the fragment is stored independently such that we still index into the correct location in the voxel grid. Here's a snippet of the geometry shader that performs this projection:

<pre><code>
	void main() {
	
		const vec3 p1 = GeomPos[1] - GeomPos[0];
		const vec3 p2 = GeomPos[2] - GeomPos[0];
		const vec3 p = abs(cross(p1, p2)); 
		for(uint i = 0; i < 3; ++i){
			FragPos = GeomPos[i];
			NormalFrag = NormalGeom[i];
			TexCoordsFrag = TexCoordsGeom[i];
			FragPosLightSpace = GeomPosLightSpace[i];
			if(p.z > p.x && p.z > p.y){
				gl_Position = projection * view * vec4(FragPos.x, FragPos.y, FragPos.z, 1);
			} else if (p.x > p.y && p.x > p.z){
				gl_Position = projection * view * vec4(FragPos.z, FragPos.y, FragPos.x, 1);
			} else {
				gl_Position = projection * view * vec4(FragPos.x, FragPos.z, FragPos.y, 1);
			}
			EmitVertex();
		}
		EndPrimitive();
	}
</code></pre>

							Most of the outputs are simply pass-through, but note the calculation of gl_Position. After finding the normal of the primitive (p), we find its dominant axis and orient it accordingly to generate the most possible fragments.
							</p>

							<p>At this point we are most of the way there to voxelizing our scene. We can calculate the stored color using the same method as would be used in our lighting pass. As an important aside, since we are using the probe method mentioned earlier, 
							   we 
							</p>
							<div class="youtube-video-container">
								<iframe width="280" height="157" src="https://www.youtube.com/embed/s5Irf37hLLA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
							</div>
						</div>
					</div>

				<!-- Footer -->
				<footer id="footer">
					<div class="inner">
						<section>
							<h2>Follow</h2>
							<ul class="icons">
								<li><a href="https://github.com/hallmluke" class="icon brands style2 fa-github"><span class="label">GitHub</span></a></li>
								<li><a href="https://www.linkedin.com/in/lukemhall/" class="icon brands style2 fa-linkedin"><span class="label">LinkedIn</span></a></li>
								<li><a href="https://www.youtube.com/channel/UCUrmUJxlNLHAPSd9ZVT69EA" class="icon brands style2 fa-youtube"><span class="label">YouTube</span></a></li>
							</ul>
						</section>
					</div>
				</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>