<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Voxel Cone Tracing | Luke Hall</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<header id="header">
					<div class="inner">

						<!-- Home -->
						<a href="index.html" class="logo fas fa-home">
							<!-- <span class="symbol"><img src="images/logo.svg" alt="" /></span> -->
						</a>

						<!-- Nav -->
							<nav>
								<ul>
									<li><a href="#menu">Menu</a></li>
								</ul>
							</nav>

					</div>
				</header>

			<!-- Menu -->
				<nav id="menu">
					<h2>Menu</h2>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="about.html">About</a></li>
						<li><a href="portfolio.html">Portfolio</a></li>
					</ul>
				</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Voxel Cone Tracing</h1>
							<h4>Last Updated: 5/4/22</h4>
							<span class="image main"><img src="images/vctheader.png" alt="" /></span>
							<h2>What is Voxel Cone Tracing?</h2>
							<p>Voxel Cone Tracing (VCT) is a technique originally proposed by NVIDIA researchers in 2011 in <a href="https://research.nvidia.com/sites/default/files/pubs/2011-09_Interactive-Indirect-Illumination/GIVoxels-pg2011-authors.pdf">this paper.</a>
							This method of calculating indirect lighting has been mostly sidelined recently with the advent of hardware-accelerated raytracing (such as RTX) allowing more sophisticated methods of calculating GI--such as <a href="https://jcgt.org/published/0008/02/01/">ray-traced irradiance fields</a>, a topic I may wish to explore in the future.
							However, I had never implemented a GI system in a renderer before, and VCT interested me for a few reasons. First, I (sadly) do not have a GPU with hardware-accelerated raytracing at the moment. That eliminated many options for me. Similarly, I wanted a system that could run smoothly (~60 FPS) on older, mid-range hardware (such as my 1050Ti).
							Second, I found that while some implementation details are tricky, the concept of VCT is fairly intuitive. As such, I set out to implement a (simplified) version of the paper mentioned above.</p>

							<p>VCT can be broken down into two stages. The first is the voxelization of the scene and its illumination into memory. For those unfamiliar, a voxel is simply a value in a 3D grid. So to voxelize the scene is to take the scene and store all relevant information (illumination, normal) into this grid structure. This allows for 
								an efficient lookup for illumination at any given point. The second stage is to sample this data to calculate indirect illumination. For each pixel, in addition to the direct illumination calculations, we can perform additional calculations to sample nearby voxels for their illumination data, allowing us to light that pixel accordingly.
							</p>

							<h2>Voxelization</h2>
							<p>So how do we even begin take our scene and convert it into this grid structure? What part of the scene are we voxelizing? Moreover, what is this grid structure in memory? For the first question, we can actually make use of the fixed-function graphics pipeline. By that I mean, we can treat this almost like an additional render pass, but instead of
								our output being a framebuffer texture target, our fragments can instead store values in a buffer. Below is a small snippet of a GLSL fragment shader that demonstrates what I mean.
								<pre><code>
...							
struct VoxelType
{
    uint color;
    uint normal;
};

layout (std140, binding = 1) buffer VoxelBuffer {
    VoxelType voxels[];
};

...

void main() {

    ivec3 voxel = scaleAndBias(FragPos);

	... 
    uint id = flatten3D(uvec3(voxel), uvec3(GISubdiv));
    atomicMax(voxels[id].color, encodedColor);
    atomicMax(voxels[id].normal, encodedNormal);

}
								</code></pre>

							In the above code, for each fragment we're storing values using the atomicMax operation (more on this later) in this buffer that is representative of our voxel grid. Based on the position of our fragment, we can index into this buffer and store our information. In this example, we're storing an encoded value of our color and the normal of the surface.
							</p>

							<p>I mentioned that we could use the fragment's global position to index into this structure. How do we go from a global position of a fragment to an index in this buffer? Well, it depends on your desired implementation. There are two ways to think about where this grid lives in 3D space.
								The first is to have this grid be relative to the current view/camera. There are several possible advantages to this. We would not need to manually place bounds for our voxel. We would only be voxelizing data we need for the current frame, not values that may not get seen. However, this would require re-voxelization every frame, as any change in view would invalidate our previous voxel data.
								The second, and the method I chose that is also used by <a href="https://docs.godotengine.org/en/latest/classes/class_voxelgi.html">popular game engine Godot</a>, is to place "probes" (bounds for the grid) into the scene manually for regions we wish to voxelize. This also allows us to simply voxelize offline or at load-time, greatly reducing the computation required. This, of course, has the obvious drawback that GI is static in the scene. Depending on the capability of the underlying hardware the renderer could either
								use the "static" data or revoxelize every frame. My definition of a probe is fairly simple:
<pre><code>
class GIProbe {
	public:
		std::shared_ptr&#60;Texture3D&#62; VoxelTex;
		uint32_t Subdiv = 128;
		float Energy = 1.0f;

		GIProbe() = default;
};
</code></pre>
							The above is a component that can be attached to a scene object with its own position/scale. The position/scale represent where it is in the scene. It contains a pointer to a 3D texture that the previous buffer is converted into (more on this later). The "Subdiv" represents the level of detail
							of the probe. For example, a probe with 128 Subdiv would be a 128 x 128 x 128 voxel grid. The "Energy" is a modifier to increase the effect of the indirect lighting for stylized/physically implausible results.
							</p>

							<p>Now that we know the probe's position, scale and subdivisions, we can begin the voxelization process. We first construct an orthographic projection matrix that covers the bounds of the voxel probe and a view matrix looking toward the center of the probe in the -Z direction.
								This is fairly trivial to do using GLM's ortho and lookat functions:
<pre><code>
glm::mat4 voxelProjection = glm::ortho(-scales[i].x, scales[i].x, -scales[i].y, scales[i].y, -scales[i].z, scales[i].z);
glm::mat4 voxelView = glm::lookAt(glm::vec3(0.0f, 0.0f, scales[i].z / 2), positions[i], glm::vec3(0.0f, 1.0f, 0.0f));
</code></pre>

							However, we have a problem. Rendering the scene from only a single view will fail to produce fragments for some triangles that should be accounted for. For example, consider a triangle whose normal is straight up. The graphics pipeline will not produce fragments for this triangle,
							and as such, will not store a value in the buffer. A naive solution would be to render the scene from three directions for each axis. This would work but would be incredibly inefficient. A better solution is to project each triangle onto its dominant axis in a geometry shader
							such that each triangle produces the maximum number of fragments. We should ensure that the actual position of the fragment is stored independently such that we still index into the correct location in the voxel grid. Here's a snippet of the geometry shader that performs this projection:

<pre><code>
	void main() {
	
		const vec3 p1 = GeomPos[1] - GeomPos[0];
		const vec3 p2 = GeomPos[2] - GeomPos[0];
		const vec3 p = abs(cross(p1, p2)); 
		for(uint i = 0; i < 3; ++i){
			FragPos = GeomPos[i];
			NormalFrag = NormalGeom[i];
			TexCoordsFrag = TexCoordsGeom[i];
			FragPosLightSpace = GeomPosLightSpace[i];
			if(p.z > p.x && p.z > p.y){
				gl_Position = projection * view * vec4(FragPos.x, FragPos.y, FragPos.z, 1);
			} else if (p.x > p.y && p.x > p.z){
				gl_Position = projection * view * vec4(FragPos.z, FragPos.y, FragPos.x, 1);
			} else {
				gl_Position = projection * view * vec4(FragPos.x, FragPos.z, FragPos.y, 1);
			}
			EmitVertex();
		}
		EndPrimitive();
	}
</code></pre>

							Most of the outputs are simply pass-through, but note the calculation of gl_Position. After finding the normal of the primitive (p), we find its dominant axis and orient it accordingly to generate the most possible fragments.
							And of course to ensure fragments are generated for all triangles, we ensure that back-face culling is not enabled and depth testing is not performed.
							</p>

							<p>At this point we are most of the way there to voxelizing our scene. We can calculate the stored color using the same method as would be used for direct illumination in our lighting pass. As an important aside, since we are using the probe method mentioned earlier, 
							   we will need to create an additional shadow map for our directional light (if one is present) specifically for voxelization. From here, we pack our color and normal information into unsigned integers. As mentioned earlier, we store these values using atomic operations.
							   Many implementations opt to keep a moving average of values, using an atomic add operation and eventually dividing by the total number of fragments evaluated. I opted for a simpler approach of only storing the maximum values for a given voxel. 
							</p>

							<p>Finally, we need to take the information that is stored in this buffer, unpack it into a 3D texture we can sample from and generate mipmaps for this texture. The unpacking and storing can be done in a compute shader that is executed after the voxelization process. Here's a snippet of the relevant compute shader:
<pre><code>
	void main() {

		const uint globalInvocationIndex = getGlobalInvocationIndex();
		VoxelType voxel = voxels[globalInvocationIndex];
	
		vec4 color = DecodeColor(voxel.color);
	
		imageStore(voxelTex, ivec3(gl_GlobalInvocationID), color);
	
	}
</code></pre>
							</p>

							<p>We have now voxelized the part of our scene contained in our GI probe. To ensure we're getting the results we expect, it would be wise to visualize our voxel probe. I wrote a simple ray-marching shader with nearest filtering on the 3D texture to visualize it. It's certainly not perfect as it does produce some odd shimmering, but for debug
								visualization purposes it gets the job done. The results are shown at this end of the video at the bottom of this article. RenderDoc was absolutely invaluable as well when trying to troubleshoot this voxelization process.
							</p>

							<h2>Cone Tracing</h2>
							<p>At last we are at the point where we can actually use this data we've spent so much time generating. To gather radiance for a particular pixel, we cone trace in many directions and average our result. A single cone trace involves choosing a direction, taking small, discreet steps along that direction, sampling our voxel grid at each point,
								and stopping when we've either reached a specified occlusion value (we've hit a surface) or reached a maximum distance. This at first may sound suspiciously like ray-marching. Where does the "cone" part come into play?
							</p>

							<p>The difference is that we sample from different mip levels depending on how far we are from our origin point. To determine which mip level we sample from, we need to calculate the diameter of our cone at each step. We then choose the smallest mip level such that the voxels represent a size bigger than that diameter. Here is a snippet of the cone tracing process:
<pre><code>
	while(dist < maxDistance && alpha < 1) {

        float diameter = max(1.0f, 2 * coneAperture * dist);
        float mip = log2( diameter * GISubdiv / GIScale.x * mipModifier); 

        vec3 worldPosition = startPos + coneDirection * dist;

        ivec3 voxelPosition = scaleAndBias(worldPosition);

        vec3 voxelTexPosition = vec3(voxelPosition) / GISubdiv;

        if(mip >= 5) {
            break;
        }

        vec4 sampled = textureLod(VoxelTex, voxelTexPosition, mip);

		...
	}
</code></pre>

							For diffuse global illumination, we want to sample enough cones such that we roughly cover the entire hemisphere of the surface. We can achieve higher quality by using more cones with smaller aperture, but it is more expensive to do so.
							For specular global illumination (glossy/reflective), we actually only need to sample one cone. The direction of the cone is the reflection of the viewing angle off the surface and the aperture can be adjusted based on the roughness of the surface.
							Note that this does not give crisp reflections the way screen-space reflections or ray-traced reflections do, but it can be useful for rough surfaces or as a fallback.
							</p>

							<h2>Results</h2>
							<p>The video below showcases what the end result looks like using this method. For exaggerated effect, all ambient lighting has been removed, and, as shown in the video, I increase the contribution of indirect lighting to show the effect.</p>

							<div class="youtube-video-container">
								<iframe width="280" height="157" src="https://www.youtube.com/embed/s5Irf37hLLA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
							</div>

							<p>My full implementation is open source as part of my Walker Engine renderer.</p>
							<p><a href="https://github.com/hallmluke/WalkerEngine" class="button primary">Walker Engine Source</a></p>
							
							<h2>References and Further Reading</h2>
							<ul>
							<li><a href="https://research.nvidia.com/sites/default/files/pubs/2011-09_Interactive-Indirect-Illumination/GIVoxels-pg2011-authors.pdf">Original Paper by Crassin et al.</a></li>
							<li><a href="https://www.ogre3d.org/2019/08/05/voxel-cone-tracing">OGRE3D Blog Post</a></li>
							<li><a href="https://wickedengine.net/2017/08/30/voxel-based-global-illumination/">Wicked Engine Blog Post</a></li>
							<li><a href="https://github.com/Neconspictor/Euclid">Euclid Engine Implementation</a></li>
						</ul>
						</div>
					</div>

				<!-- Footer -->
				<footer id="footer">
					<div class="inner">
						<section>
							<h2>Follow</h2>
							<ul class="icons">
								<li><a href="https://github.com/hallmluke" class="icon brands style2 fa-github"><span class="label">GitHub</span></a></li>
								<li><a href="https://www.linkedin.com/in/lukemhall/" class="icon brands style2 fa-linkedin"><span class="label">LinkedIn</span></a></li>
								<li><a href="https://www.youtube.com/channel/UCUrmUJxlNLHAPSd9ZVT69EA" class="icon brands style2 fa-youtube"><span class="label">YouTube</span></a></li>
							</ul>
						</section>
					</div>
				</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>